\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\setlength{\parindent}{0mm}
\newcommand{\ep}{\epsilon}
\newcommand{\la}{\lambda}
\newcommmand{\nc}{\newcommand}
\nc{\openm}{\begin{pmatrix}}
\nc{\closem}{\end{pmatrix}}

\begin{document}
Name: Hall Liu

Date: 1/24/13
\vspace{1.5cm}

Dirchlet distribution: useful for generalizing a beta distribution, multivariate version of it. Useful for analyzing data that has several RVs which sum up to something less than $1$. Recall that if we have two gamma distros $X$ adn $Y$, then $X/Y$ is essentially a beta distro. Try this as exercise. Specifically, let $x\sim Gamma(y,\lambda), Y\sim Gamma(s,\lambda)$, and ifnd joint density of $X/(X+y)$ and $X+Y$. $X$ and $Y$ are indep.

The dirchlet distribution is actually defined in a similar way. Supp. $X\sim Gamma(r,\lambda)$, $Y\sim Gamma(s,\lambda)$, $Z\sim Gamma(t,\lambda)$. Consider $U=X/(X+Y+Z)$, $V=Y/(X+Y+Z)$. Then want to find joint distro of $U,V$. (if we want more components, we just throw in more gammas) Know joint distro of $X,Y,Z$. Want the trivariate transformation, so take $W=X+Y+Z$. Now want to transform to $U,V,W$. Afterwards we can integrate out $W$. The inverse transformation is is $X=UW$,$Y=VW$,$Z=W(1-U-V)$. The jacobian is 
$$\left|\begin{pmatrix}
W&0&-W\\
0&W&-W\\
U&V&1-U-V\\
\end{pmatrix}\right|=W(W(1-U-V)+WV)-W(-WU)=W^2$$

Specify values of $U,V,W$. $W$ ranges to $\infty$, but $U+V$ is between 0 and 1 and so are each of them individually. Now plug into that formula. the joint distro is 
$$f_{U,V,W}=f_{X,Y,Z}(UW,VW,W(1-U-V))W^2=\frac{\lambda^r}{\Gamma(r)}u^{r-1}w^{r-1}e^{-\lambda uw}\cdot\frac{\lambda^s}{\Gamma{s}}v^{s-1}w^{s-1}e^{-\lambda vw}\cdot\frac{\lambda^t}{\Gamma(t)}(1-u-v)^{t-1}w^{t-1}e^{-\lambda w(1-u-v)}=\frac{\lambda^{r+s+t}}{\Gamma(r)\Gamma(s)\Gamma(t)}w^{r+s+t-1}u^{r-1}v^{s-1}(1-u-v)^{t-1}^{-\lambda w}=u^{r-1}v^{s-1}(1-u-v)^{t-1}\frac{\Gamma(r+s+t)}{\Gamma(r)\Gamma(s)\Gamma(t)}\frac{\lambda^{r+s+t}}{\Gamma(r+s+t)}w^{r+s+t-1}e^{-\lambda w}$$
The separation immediately implies independence of $W$, and integrating out gives the joint density of $U,V$ which is $u^{r-1}v^{s-1}(1-u-v)^{t-1}\frac{\Gamma(r+s+t)}{\Gamma(r)\Gamma(s)\Gamma(t)}$. This joint density is known as the Dirichlet density. It looks a lot like the beta distribution. It actually turns out that the marginal density of either U or V is the beta. 

Marginal density of $U$: integrate from $0$ to $1$
$$\frac{\Gamma(r+s+t)}{\Gamma(r)\Gamma(s)\Gamma(t)}u^{r-1}\int_0^{1-u}v^{s-1}(1-u-v)^{t-1}dv$$
substitute $v=x(1-u)$. Then $$\frac{\Gamma(r+s+t)}{\Gamma(r)\Gamma(s)\Gamma(t)}u^{r}\int_0^{1}x^{s-1}(1-u)^{s-1}(1-u-x(1-u))^{t-1}dv=\frac{\Gamma(r+s+t)}{\Gamma(r)\Gamma(s)\Gamma(t)}u^{r-1}\int_0^{1}x^{s-1}(1-u)^{s-1}(1-u)^{t-1}(1-x)^{t-1}(1-u)dx=frac{\Gamma(r+s+t)}{\Gamma(r)\Gamma(s)\Gamma(t)}u^{r-1}(1-u)^{s+t-1}\int_0^{1}x^{s-1}(1-x)^{t-1}dx$$
Now the integral is the beta form, and $U$ ends up as a beta distro with parameters $r,s+t$. 

Next: back to multivariate normal distro. Interpretation in terms of linear algebra. Bivariate first, since it's simpler. Take $\vec{X}=\openm X_1\\X_2\closem$ and $\vec{\mu}=E\vec{X}=\openm EX_1\\EX_2\closem$ and $\sigma=\openm\sigma_{11}&\sigma_{12}\\\sigma{21}&\sigma{22}\closem=\openm var(X_1)&Cov(X_1,X_2)\\Cov(X_2,X_1)&var(X_2)\closem=\openm\sigma_1^2&\sigma_1\sigma_2\rho\\\sigma_1\sigma_2\rho&\sigma_2^2\closem$. Actually the same as saying $E((X-E(X))(X-E(X))^{T})$. 

Then, the bivariate density of $X_1,X_2$ can be expressed as $f_{X_1,X_2)=\frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left(\frac{-1}{2(1-\rho^2)}\cdots\right)$. This can alternatively be expressed in matrix form as $$\frac{1}{2\pi\sqrt{\det\Sigma}}\exp\left(-1/2(X-\mu)^{T}\Sigma^{-1}(X-\mu)\right)$$. Derivation is straightforward but involves a lot of annoying manipulations. This is an alternative way to define a multivariate normal distro: give the mean vector $\vec{\mu}$ and the cov matrix $\Sigma$. However, the $1/(2\pi)$ out front needs to have a power of $p/2$ on it for a $p$-dimensional distro. Notation is $X\sim N(\mu,\Sigma)$. Supp. $X$ is such a multivar. RV with dimension $p$. Examine distro of $AX$, where $A$ is a constant matrix. Turns out $AX\sim N(A\mu, A\Sigma A^{T})$. 

First let's prove it for a full-rank square matrix. Let $Y=AX$, and $X=A^{-1}Y$. The Jacobian of the transformation is $\det(A^{-1})$ which is $1/\det(A)=1/\det(A^T)=\frac{1}{\sqrt{\det(AA^T)}$. Then $$f_Y(y)=f_X(A^{-1}y)\frac{1}{\sqrt{\det(AA^T)}=\frac{1}{\sqrt{2\pi}^p\frac{1}{\det\Sigma}e^{-1/2(A^{-1}Y-\mu)^T\Sigma^{-1}(A^{-1}Y-\mu)}\frac{1}{\sqrt{\det(AA^T)}$$
Squeeze the determinants together. then 
$$\frac{1}{\sqrt{2\pi}^p\frac{1}{\det(A\Sigma^T}\exp\left(-1/2(y-A\mu)^T(A^{-1})^T\Sigma^{-1}A^{-1}(y-A\mu)\right)$$
Collapse the middle mess of matrices and we get what we wanted. 

\section*{01/31/13}
Comments on the t-test: the data set was 
\begin{array}{cc}
$x_1$&$y_1$\\
\hline
191&179\\
195&201\\
etc&etc
\end{array}

$x_1$ is head length of first son.., $y_1$ is the head length of the second. Both are normal, with $x_1\simN(\mu_1,\sigma_1^2),x_2\sim N(\nu,\tau_1^2)$. This is bivariate norma, maybe with some correlation. Look at the difference $U=X_1-Y_1$. $U\sim N(\mu_1-\nu_1,\sigma_u^2)$, where the variance is the sum of the two other variances minus $2\rho\sigma\tau$. Can compute sample mean on $U$, turns ot to be $1.88$. sample SD of $U$ us 7.93. The $n$ is 25. The $t$-statistic is $\frac{\sqrt{n}(\var{U}-(\mu_1-\nu_1)}{s_u}$, follows a $t$-distribution with $n-1$ dof. To test the hypothesis that $\mu_1=\nu_1$, we have that under $H_0$, $T=\frac{\sqrt{n}\var{U}}{s_n}=1.2975$ follows a $t_24$ distribution. Then, find probabilities of this value from the distributon. Turns out there's not much siginficance. 

What if we wanted to know if there's any correlation? If there was no correlation above, then we could have done something stronger. To test correlation, we need a distribution for the sample correlation. The actual formula is really god-awful ugly. Instead we'll look for asymptotic distribution of sample correlation. In the above dataset, the correlation as 0.71. 

suppose $(X_1,Y_1),(X_2,Y_2),...$ are $n$ indep bivariate normal RVs, with all of them being standardized with unknown correlation $\rho_i$. Standardinzing doesn't change the correlation. Formula for sample correlation is $\hat{\rho}=\frac{1/n\sum_1^nX_i-\bar{X})(y_i-\bar{Y}}{\sqrt{1/n\sum(x_i-\bar{x})^2}\sqrt{1/n\sum(Y_i-\bar{Y})^2}}4$. What we want to derive is that the asymptotic distribution of $\sqrt{n}(\hat{\rho}-\rho)$ converges in distribution to $N(0,(1-\rho^2)^2)$. We probably want to use CLT at some point. That means showing that $\hat{\rho}$ is some sort of sum or average of RVs. 

Simplify $\hat{\rho}$. Idea is to work with the means being $0$ first: $\hat{\rho}=\frac{1/n\sumX_iY_i}{\sqrt{1/n\sumX_i^2}\sqrt{1/nY_i^2}}$. Justification is that if we look at the numerator, the cross terms disappear and the inside of the sum is left with $X_iY_i-\bar{X}\bar{y}$. As $n$ increases, variance of the bar terms goes to $0$, so the latter term is ignorable for large $n$. 

Let's try to get back to independent RV now. So we want to wrrite thte $Y_i=\rho X_i+\sqrt{1-\rho^2}\ep_i$. Then, $\hat\rho=\frac{1/n\sumX_i(\rhoX_i_\sqrt{1-\rho^2}\ep_i}{...}$, denom still the same. Now separate out. There's a $\rho/n\sum X_i^2$ and a $\sqrt{1-\rho^2}\cdot1/n\sum X_i\ep_i$. The first term can cancell out with the denom term.

Now look $\hat{\rho}-\rho$. This is $$\rho\frac{\sqrt{1/n\sum X_+i^2}-\sqrt{1/n\sum Y_i^2}}{\sqrt{1/n\sum Y_i^2}}+\sqrt{1-\roh^2}\frac{1/n}...$$
Rationalize the numerator of the first term. turns into $\frac{1/n\sum X_i^2-1/n\sum Y_i^2}}{\sqrt{1/n\sum Y_i^2}(\sqrt{1/n\sum X_i^2}+\sqrt{1/n\sum Y_i^2})}+(1-\rho^2)\frac{1/n\sum X_i\ep_i}{
\end{document}
