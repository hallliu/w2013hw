\documentclass{article}
\usepackage[margin=0.2in]{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\newcommand{\nc}{\newcommand}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\nc{\pd}{\partial}
\nc{\del}{\nabla}
\nc{\ep}{\epsilon}
\DeclareMathOperator{\tr}{tr}
\begin{document}
\small
Defn of independence: $P(A\cap B)=P(A)P(B)$. Conditional probability is $P(A|B)=\frac{P(A\cap B)}{P(B)}$, which gives rise to $P(B_j|A)=\frac{P(A|B_j)P(B_j)}{\sum P(A|B_i)P(B_i)}$, Baye's rule. Bernoulli RVs are the binary ones with parameter $p$, binomial random variables are the ones that count successes in indep. Bernoulli, with pmf $\binom{n}{k}p^k(1-p)^{n-k}$. Geometric is the number of trials before first success, pmf is $p(k)=(1-p)^{k-1}p$. Negative binomial is the sum of a bunch of geometrics, has parameters $r$ and $p$. pmf is $\binom{k-1}{r-1}p^r(1-p)^{k-r}$. 
Poisson is the cts-time version of gemoetric, can be derived by taking $n\to\infty$ with $np=\lambda$. pmf is $\frac{\lambda^k}{k!}e^{-\lambda}$. Exponential density is cts, has pdf $\lambda e^{-\lambda x}$. Normal distr. is $\frac{1}{\sigma\sqrt{2\pi}}e^{-(x-\mu)^2/2\sigma^2}$. Gamma distribution is $\frac{\lambda^a}{\Gamma(\alpha)}t^{\alpha-1}e^{-\lambda t}$ with parameters $\lambda$ and $\alpha$. The $\chi^2_n$ distribution is $\Gamma(n/2,1/2)$ To transform RVs according to a function, the formula is $f_Y(y)=f_X(g^{-1}(y))\big|\frac{d}{dy}g^{-1}(y)\big|$, where g has to be invertible. To generate random numbers with a given cdf $F$, apply $F^{-1}$ to a unif. distribution. Useful fact is that the sum of a bunch of exponential distributions ends up as a gamma distribution with $\lambda=n$.

Mean-variance pairs for distributions: binom: $(np,np(1-p))$. geometric: $(1/p, (1-p)/p^2)$. Negative binomial: $(pr/(1-p),pr/(1-p)^2)$. Poisson: $(\lambda,\lambda)$. Gamma: $(\alpha/\lambda, \alpha/\lambda^2)$.

For joint distributions, extract marginal pdfs by integrating out the other variables over the domain. Extract the marginal cdf by integrating that over part of the domain. Marginals don't uniquely determine a joint - $F(x)G(y)(1+\alpha(1-F(x))(1-G(y)))$ is a cdf that has marginal cdfs $F$ and $G$. 
Independence works the same way as in other RVs, if the joint factors into the product of marginals. Conditional distr. given by $p_{X|Y}(x|y)=\frac{p_{XY}(x,y)}{p_Y(y)}$. Cts case is similar. To transform joint RVs, the formula is $f_{UV}(u,v)=f_{XY}(h_1(u,v),h_2(u,v))|J^{-1}(h_1,h_2)|$, where the $J^{-1}$ is the inverse of the Jacobian and $x=h_1(u,v)$ and same for $y$. 

Expected value is $\int xf_X(x)$ with obvious discrete counterpart. Integral or sum must not diverge. Markov's ineq states that if $X$ is nonnegative and $EX$ exists then $P(X\geq t)\leq E(X)/t$. proof trivial. Expected value of a function of a RV is $E(g(X))=\int g(x)f_X(x)$. Proof by examining preimages of each $y$ in image of $g$ and writing $p_Y(y)=\sum_{g(x)=y}p_X(x)$. Still holds in multivariate case. Linear combos of $E(X_i)$ 
are the same linear combinations. $E$ is linear. Variance is defined as $E((X-E(X))^2)$, or $\int(x-\mu)^2f_X(x)$. Alternate form is $E(X^2)-(E(X))^2$, proven by expanding. Chebyshev's ineq is $P(|X-\mu|>t)\leq\frac{\sigma^2}{t^2}$. Mean squared error is $E((X-x_0)^2)$, where $x_0$ is the true value. Covariance of two RVs is $E((X-\mu_x)(Y-\mu_y))$. Alternate form is $E(XY)-E(X)E(Y)$. 
Covariance is fully bilinear, with Cov$(U,V)=\sum_i\sum_jb_ib_j\text{Cov}(X_i,Y_j)$. Note $var(X)=Cov(X,X)$, so identities for variance follow. Correlation is $\frac{Cov(X,Y)}{\sqrt{var(X)var(Y)}}$. Correlation is always in $[-1,1]$, and it's $\pm1$ iff $P(Y=a+bX)=1$. Show by considering variance of $\frac{X}{\sigma_X}\pm\frac{Y}{\sigma_Y}$.

Conditional expectation is $E(Y|X=x)=\Sigma_yyp_{Y|X}(y|x)$. This is essentially the mean of the conditional distr. We have $E(Y)=E(E(Y|X))$, where the inner $E$ is taken over $Y|X$ and the outer is taken over $X$. Derive by interchanging order of summation. Also $var(Y)=var(E(Y|X))+E(var(Y|X))$, again where the inners are over $Y|X$ and the outers are over $X$. $var(Y|X)=E(Y^2|X)-(E(Y|X))^2$. 
If we want to minimize the MSE when approximating a RV $Y$ by another RV $X$, we can use $h(X)=E(Y|X)$. For linear predictors, we consider $E(Y-\alpha-\beta X)^2)$, expand, and get two terms of which only one has $\alpha$. If we let $\alpha=\mu_Y-\beta\mu_X$, the second one goes to $0$, and the first one goes to $\sigma_Y^2+\beta^2\sigma_X^2-2\beta\sigma_{XY}$. Minimize wrt $\beta$ and get $\beta=\rho\frac{\sigma_Y}{\sigma_X}$ where $\rho$ is the corr. coef.

Law of large numbers states that for $X_i$ a seq of iid RVs, $P(|\sum_i^nX_i-\mu|>\epsilon)\to0$ as $n\to\infty$ for all $\epsilon>0$. Derive by calculating $E(\conj{X_n})$ and its var, then compare using Chebyshev.

Central limit theorem: Let $X_1,\ldots,X_n$ be iid with mean $0$ and variance $\sigma^2$. Then the cdf of $\frac{S_n}{\sigma\sqrt{n}}$ approaches the standard normal cdf pointwise (for all useful purposes, uniformly as well).

Doing Bayesian inference: Start with prior distribution $f_\Theta(\theta)$ of the `random variable' $\theta$ (usually taken to be uniform). Also know that the data are likely to be distributed according to a pdf depending on $\theta$, so $f_{X|\Theta}(x|\theta)$. Multiply them together to get the joint distr, then integrate out $\theta$ to get the marginal distr. of $X$. We can then find the distr of $\Theta$ given the data $X$ as the quotient of the joint by the X marginal distr.

Maximum likelihood estimators: Define the likelihood function $L(\theta)=f(x_1,\ldots,x_n|\theta)$, where $f$ is a pdf that varies depending on $\theta$. Maximum likelihood estimator is the value of $\theta$ that maximizes $L$. In practice when we have iid data, $L$ is a product of individual pdfs, so usually want to take $\log L(\theta)=l(\theta)$ to convert the product into a sum. DON'T FORGET TO TAKE THE LOG. DON'T JUST TAKE APART THE PRODUCT. 

Theorems on MLEs: Define $I(\theta)=E\left(\pd{}{\theta}{}\log f(X|\theta)\right)^2$. When $f$ is nice, this is equal to $-E\left(\pd{}{\theta}{2}\log f(X|\theta)\right)$. Also, as sample size increases, the distribution of the MLE tends to normal with mean $\theta_0$ (the true parameter) and variance $\frac{1}{nI(\theta_0)}$. One notable condition is that the parameter space is open. THE FORMULA FOR I SHOULD BE DONE WITH ONE X, NOT ALL OF THEM. THE DERIVATIVE INSIDE IS NOT $l''$. The variance should eventually be expressed in terms of the true parameter.

Likelihood ratio test: Compute likelihoods of the two hypotheses given the data and take the ratio, taking prior probabilities into account. Accept $H_0$ if the ratio of $L(H_0)/L(H_a)>1$ and reject otherwise. This is given by the equation $\frac{P(H_0|x)}{P(H_a|x)}=\frac{P(H_0)}{P(H_a)}\frac{P(x|H_0)}{P(x|H_a)}$, and it's equivalent to setting some critical value for $\frac{P(x|H_0)}{P(x|H_a)}$ for rejection.

A few terms: Type I error is rejecting $H_0$ when it's true. Type II error is acception $H_0$ when it's false. Power is probability of rejection when $H_0$ is false. Significance level is the probability of a type I error.

Neyman-Pearson lemma: States that the likelihood ratio test is the most powerful test, in that any other test that has a lower significance level also has a lower power. Related to the concept of uniformly most powerful, where a test is most powerful for all variants of $H_a$. Usually shown by deriving the likelihood ratio as a function of $\conj{X}$, using the Neyman-Pearson lemma to show that the most powerful test for some variant of $H_a$ is reject for $\conj{X}>x_0$, then choosing $x_0$ based on $P(\conj{X}>x_0)=\alpha$ assuming $X$ is distributed according to the null hypothesis. This usually won't depend on the alternative if we're doing it that way.

Generalized LRT: When $H_0$ and $H_a$ are composite, do the LRT a bit differently. For the $H_0$ case, use the MLE of the parameter in question (while remaining within the bounds of the hypothesis) as the thing to plug into the likelihood, and use the MLE of the whole space on the bottom. The GLR is $\Lambda=\frac{\max_{\theta\in H_0}L(\theta)}{\max_{\theta\in\Omega}L(\theta)}$. Distribution of $-2\log\Lambda$ is $\chi^2$ with degree of freedom equal to the dimension of the parameter space minus the dimension of the null hyp space.

Sometimes we need to find $\theta$ subject to a constraint when doing the GLRT. Use Lagrange multipliers. The concept here is that maximization happens when the gradient of the function and the gradient of the constraint are in the same direction, so if $f$ is the function and $g$ is the constraint, we get the equation $\del f=\lambda\del g$, which provides $n$ equations on $n+1$ variables ($\lambda$ included). The last eqn is given by the constraint itself.

Confidence intervals: the duality with hypothesis tests is: if a test at level $\alpha$ for $H_0:\theta=\theta_0$ accepts for statistics in $A(\theta_0)$, then the confidence interval $\{\theta:X\in A(\theta)\}$ is a $1-\alpha$ CI.

Poisson processes: Have $n$ trials, each a realization of a Poisson RV. Each has a pdf given by the poisson pmf. The MLE for the parameter is the average number of counts in each trial. Using this MLE, we can find the number of expected counts per cell. Poisson processes can be distributed in space as well as in time. Poisson processes are completely characterized by independence of disjoint intervals, stationary over time (or space), and no clumping (probability that two or more events occur in interval of length $\delta$ is $o(\delta)$.

Goodness of fit tests for Poisson: Counts are grouped into cells. Under the null hypothesis, each cell has some probability. The distributions of counts in the cells is multinomial. The $X^2$ statistic is given by $\sum\frac{(x_i-np_i(\hat{\theta}))^2}{np_i(\hat{\theta})}$. This is observed minus expected, squared, over expected, where expected is the MLE for  This is approximately equal to $-2\log\Lambda$ under the GLRT. Has degree of freedom two less than number of cells. In general, if we have $m$ cells and a estimator parameter that is $k$ dimensional, $-2\log\Lambda$ is $\chi^2_{m-k-1}$. 

Variance-stabilizing transforms: motivation is to construct confidence interval for poisson RVs. CLT says that $\sqrt{n}(\bar{X}-\lambda)\to N(0,\lambda)$ in distribution. CI is given by $P(-1.96\sqrt{\lambda}<\sqrt{n}(\bar{X}-\lambda)<1.96\sqrt{\lambda})=0.95$. Too hard to solve for $\lambda$. The var. stab. transform is a function $\phi$ such that $\sqrt{n}(\phi(\hat{\theta})-\phi(\theta))$ is asymptotically normal with variance $1$. Know that $\phi=\int\frac{1}{\sqrt{g(\theta)}}d\theta$, where $g(\theta)$ is the function of $\theta$ that is the original variance (e.g. $g(\theta)=\theta$ in poisson case).

Transforming bi/multivariate distributions: If we have $U=f(X,Y),V=g(X,Y)$ and $X,Y$ distributed according to $h_{X,Y}(x,y)$, then the distribution of $U,V$ is given by $h_{X,Y}(f^{-1}(U,V),g^{-1}(U,V))\frac{\partial X,Y}{\partial U,V}$, where $f^{-1}$ and $g^{-1}$ are the inverses of $f,g$. 

Bi/multivariate normal: Bivariate $(U,V)$ is actually a linear transformation of the indep std normal pair $(X,Y)$. If the transformation matrix is $A=\openm a&b\\c&d\closem$, then the resulting pair has covariance matrix $AA^T$. In addition, given two arbitrary binomial normal RVs $U\sim(\mu,\sigma^2)$ and $V\sim(\nu,\tau^2)$ with correlation $\rho$, the pair $U,V$ is the transformation $\openm\sigma &0\\\tau\rho &\tau\sqrt{1-\rho^2}\closem$ applied to a pair of indep std normal RVs. 

$t$ and $F$ distributions: $X$ has a $t_n$ distribution if $X\sim\frac{Z}{\sqrt{U/n}}$ where $Z$ is std normal and $U\sim\chi^2_n$. Has density $\frac{\Gamma((n+1)/2)}{\sqrt{n\pi}\Gamma(n/2)}(1+t^2/n)^{-(n+1)/2}$. $F_{n,m}$ distribution is $\frac{U/n}{V/m}$ where $U$ is $\chi^2_n$ and $V$ is $\chi^2_{m}$. Has density $\frac{\Gamma((m+n)/2)}{\Gamma(m/2)\Gamma(n/2)}\left(\frac{n}{m}\right)^{n/2}w^{n/2-1}(1+\frac{n}{m}w)^{-(n+m)/2}$. $t$ has mean $0$ and variance $n/(n-2)$. $F$ has mean $m/(m-2)$ and variance $\frac{2m^2(m+n-2)}{n(n-2)^2(m-4)}$.

Disrtibution of sample statistics: If $X_i\sim N(\mu,\sigma^2)$ iid , then $\bar{X}=\frac{1}{n}\sum X_i\sim N(\mu,\sigma^2/n)$ and $s^2=\frac{1}{n-1}\sum(X_i-\bar{X})^2$ is distributed like a chi-sq with $(n-1)s^2/\sigma^2\sim\chi^2_{n-1}$. Notably, $\frac{\bar{X}-\mu}{s/\sqrt{n}}\sim t_{n-1}$. Sample correlation: $\sqrt{n}(\hat{\rho}-\rho)\sim N(0,(1-\rho^2)^2)$ as $n\to\infty$. Appropriate var-stab transformation is $1/2\log\left(\frac{1+\rho}{1-\rho}\right)$. This lets us do tests on sample correlation. 

Independence results: Deviations from the mean are independent from the mean, and mean and sample standard deviation are independent.

$t$-testing: three options. 1-sample, 2-sample, and paired. 1- and 2-sample assume independence. 1-sample is based on the above result on distribution of sample variance. Use the $t_{n-1}$ distribution. 2-sample: let $X\sim N(\mu_X,\sigma_X^2)$ and $Y\sim N(\mu_Y,\sigma_Y^2)$. $\bar{X}-\bar{Y}$ is distributed like $N(\mu_X-\mu_y,\sigma_X^2/n+\sigma_Y^2/m)$. Distribution of $\frac{(\bar{X}-\bar{Y})-(\mu_X-\mu_Y)}{\sqrt{s_X^2/n+s_Y^2/m}}$ is approximately a $t$ distro with $\nu=\frac{(s_X^2/n+s_Y^2/m)^2}{((s_X^2/n)^2/(n-1)+(s_Y^2/m)^2/(m-1))}$. If we can assume that $\sigma_X=\sigma_Y$, then we can pool the sample variance and use $t=\frac{(\bar{X}-\bar{Y})-(\mu_X-\mu_Y)}{s_p\sqrt{1/n+1/m}}$ where $s_p^2=\frac{(n-1)s_X^2+(m-1)s_Y^2}{n+m-2}$. Paired tests go like 1-sample tests, but instead they test differences. If there is positive correlation, then the variance in $\bar{D}$ will be smaller than that of $\bar{X}-\bar{Y}$.

Constructing $t$-test -- $t$-distribution is normal RV over $\sqrt{\chi^2_n/n}$. Normal RV is $(\bar{X}-\mu)/(\sigma/\sqrt{n})$, where $\mu$ is mean under $H_0$ and $\sigma$ is true variance. By results about sample standard deviation, $(n-1)s^2/\sigma^2\sim\chi^2_{n-1}\implies s/\sigma\sim\sqrt{\chi^2_{n-1}/(n-1)}$. Dividing the normal RV by this gives $\frac{\bar{X}-\mu}{s/\sqrt{n}}\sim t_{n-1}$.

Bonferroni corrections: when running $k$ tests on $k$ hypotheses, run each test at level $\alpha/k$ where $\alpha$ is the desired overall level.

Equivalence of hypothesis tests: When proving something equivalent with LRT, find the form of the LRT statistic and say that we reject when it's bigger/smaller than some critical value, then transform the LRT statistic inside the probability thing to get the form of the test statistic. Example for $F$ test is $P(J.\log(SS_b/SS_w+1)>c)=P(SS_b/SS_w>\exp(c/J.)-1)=P\left(\frac{SS_b/(I-1)}{SS_w/(J.-I)}>\frac{J.-I}{I-1}(\exp(c/J.)-1)\right)<\alpha$.

ANOVA: One-way layout characterized by $I$ levels of one factor influencing response. For each factor, let there be $J_i$ results, for a total of $J.$ results. Each result is $Y_{ij}=\mu+\alpha_i+\ep_{ij}$, where $\alpha_i$ is effect of $i$th level and $\ep$ is normal with mean $0$ and variance $\sigma^2$. Constrain $\sum\alpha_i=0$. We have $ss_T=\sum_i\sum_j(Y_{ij}-\bar{Y}_{..})^2=\sum_i\sum_j(Y_{ij}-\bar{Y}_{i.})^2+\sum_iJ_i(\bar{Y}_{i.}-\bar{Y}_{..})^2=ss_W+ss_B$.
$ss_B/\sigma^2\sim\chi^2_{I-1}$ and $ss_W/\sigma^2\sim\chi^2_{I(J_.-1}$. Dividing the two gives a $F$-ratio that can be used to construct a $F$-test. 

For any independent sample with possibly differnt means and the same variance, $E((X_i-\bar{X})^2)=(\mu_i-\bar{\mu})^2+\frac{n-1}{n}\sigma^2$, where $\bar{\mu}=(1/n)\sum_i\mu_i$. Using this, we can get $E(ss_W)=I(J_.-1)\sigma^2$ and $E(ss_B)=\sum_iJ_i\alpha_i^2+(I-1)\sigma^2$. 

Two way layout is similar, but with factors $A$ and $B$ at their respective levels. Model is $Y_{ijk}=\mu+\alpha_i+\beta_j+\delta_{ij}+\ep_{ijk}$. Constraintsare each factor-mean summing to zero, and the interactions summing to zero along any axis. $ss_A$ and $ss_B$ are defined similarly to the one-way model here, and $ss_{AB}$, the interaction sum of squares, is $K\sum_i\sum_j(\bar{Y}_{ij.}-\bar{Y}_{.j.}-\bar{Y}_{i..}+\bar{Y}_{...})^2$. By the lemma above, expectation of $ss_A$ is $(I-1)\sigma^2+JK\sum\alpha_i^2$, $E(ss_B)$ is $(J-1)\sigma^2+IK\sum\beta_j^2$, $E(ss_{AB})=(I-1)(J-1)\sigma^2+K\sum_i\sum_j\delta_{ij}^2$, and $E(ss_E)=IJ(K-1)\sigma^2$. 

Distributions of the sum of squares in two-way: all chi-squared with different dofs under null hypothesis (all the same). $ss_E/\sigma^2$ is $IJ(K-1)$, $ss_A$ is $I-1$, $ss_B$ is $J-1$, and $ss_{AB}$ is $(I-1)(J-1)$. We can then construct a test for all $\alpha_i=0$, all $\beta_j=0$, or all $\delta_{ij}=0$ by dividing the chisqs by $ss_E$ to get a $F$-statistic. 

Randomized block for 2-way: generalization of matched-pairs to matched-n-tuples. Model it as a two-way without any interaction and $K=1$. 

Post-ANOVA testing: To test two levels of a factor against each other, use the $t$-test. Make sure to calculate variance in a reasonable way -- actual variance of difference in sample means might be higher than what I'd expect. Try construction the $t$-test by hand.

Linear regression: Simple regression: Consider pairs $x_i,y_i$ where $x_i$ are fixed and $y_i$ are random according to the model $y_i=\beta_0+\beta_1x_i+\ep_i$ with $\ep_i\sim N(0,\sigma^2)$ iid. Do this by minimizing sum of residuals squares, $\sum(y_i-\beta_0-\beta_1x_i)^2$. Calculate MLEs for $\beta_0$,$\beta_1$ to get that $\beta_1=\frac{\sum x_iy_i-\bar{Y}\sum x_i}{\sum x_i^2-\bar{X}\sum x_i}=\frac{\sum x_iy_i-n\bar{Y}\bar{x}}{\sum x_i^2-n\bar{X}^2}=\frac{ss_X}{ss_{XY}}$ and $\beta_0=\bar{Y}-\beta_1\bar{X}$. The $\beta_i$ are unbiased, and if we write $ss_{XY}=\sum(x_i-\bar{X})Y_i$ we find that variance of $\beta_1$ is $\sigma^2/ss_X$. Variance of $\beta_0$ is similarly $\sigma^2(1/n+\bar{X}^2/ss_X)$., and covariance is $-\bar{X}\sigma^2/ss_X$. We can do hypothesis testing on the $\beta_i$: sum of residuals squared is indep of $\beta_i$ and distributed like $\sigma^2\chi^2_{n-1}$. Can similarly do ANOVA $F$-test by defining $ss_T=\sum(y_i-\bar{Y})^2=\sum r_i^2+\sum(\widehat{\beta_0}+\widehat{\beta_1}x_i-\bar{Y})^2=\sum r_i^2+ss_X\sum\beta_1^2$ by substituting in expr for $\beta_0$. $ss_T$ is chisq with $n-1$ dof, $ss_E$ is chisq with $n-2$ dof, so $ss_M$, their difference, has $1$ dof, and we can make a $F$-test. 

Multiple regression: Model is $y_i=\beta_0+\beta_1x_{1i}+\cdots+\beta_px_{pi}+\ep_i$. Assume all the variances are the same, only $\ep_i$ are random. Define column vectors $y=(y_1,\ldots,y_n)$, $x_j=(x_{j1},\ldots,x_{jn})$,$\beta=(\beta_0,\ldots,\beta_p)$, and $\ep=(\ep_1,\ldots,\ep_n$. Define matrix $X$ as matrix with first column all $1$s (corresponding to $\beta_0$) and $j$th column $x_{j-1}$, for a total dimension of $n\times(p+1)$. Then $y=X\beta+\ep$. Minimizing with lease squares gives $\widehat{\beta}=(X^TX)^{-1}X^Ty$. We are guaranteed that $X^TX$ is invertible if $X$ has rank $p+1$. 

Theorems on matrices: let $z=c+Ay$, where $y$ is random and $c$ is fixed. Then $E(z)=c+AE(y)$, and the covariance matrix of $z$ is $A\Sigma_{YY}A^T$. For a random vector $x$ and a constant matrix $A$, $E(x^TAx)=\tr(A\Sigma)+\mu_TA\mu$, where $\Sigma$ is the covariance matrix of $x$ and $\mu$ is the mean vector. 

Properties of $\widehat{\beta}$: unbiased and has covariance matrix $\sigma^2(X^TX)^{-1}$. To estimate $\sigma^2$, define $P=X(X^TX)^{-1}X^T$ so the ersidual vector $e=y-\hat{y}=y-X\hat{\beta}=y-Py$. $P$ is idempotent. By computing $E(e\cdot e)$, we get $(n-p-1)\sigma^2$, so we can estimate $\sigma^2$ by $\frac{e\cdot e}{n-p-1}\sim\chi^2_{n-p-1}$. 

Doing ANOVA similar to simple regression, except $ss_M$ has dof $p$. To do prediction of average, estimate $x_0^T\beta$ (value at $x_0$) by $x_0^T\hat{\beta}$, distributed according to $N(x_0^T\beta,\sigma^2x_0^T(X^TX)^{-1}x_0)$. Prediction of an individual is the same, but with an $\ep$ added on for a extra factor of $\sigma^2$ in hte variance.

Various integrals: $\int_0^\infty x^{a-1}(1-x)^{b-1}=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$, $\int_0^\infty e^{-x^2}=\sqrt{\pi}/2$, $\int_{-\infty}^\infty e^{-ax^2+bx}=\exp(b^2/4a)\sqrt{\pi/a}$. For even $n=2k$, $\int_0^\infty x^ne^{-ax^2}=\frac{(2k-1)!!}{2^{k+1}a^k}\sqrt{\frac{\pi}{a}}$. For odd $n=2k+1$, the same integral is $\frac{k!}{2a^{k+1}}$. $\int_0^\infty x^ne^{-ax}=\frac{n!}{a^{n+1}}$. 

\end{document}
