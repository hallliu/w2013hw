\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\xbar}{\bar{X}}
\nc{\ybar}{\bar{Y}}
\begin{document}
Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{12.2}
If $I=2$, call the samples $X_1,\ldots,X_n$ and $Y_1,\ldots,Y_m$. Then, from 12.2.1,
$$s_p^2=ss_W/(n+m-2)=\frac{1}{n+m-2}\left(\sum_j(X_j-\bar{X})^2+\sum_j(Y_j-\bar{Y})^2\right)=\frac{(n-1)s_X^2+(m-1)s_Y^2}{n+m-2}=s_p^2$$
from 11.2.1 with $n=m=J$.
\subsection*{12.3}
We showed above that the denominator of the $F$-statistic is equal to the $s_p^2$ from the $t$-test. The numerator of the $F$-statistic is $ss_B/(I-1)=ss_B$, which is 
$$n\left(\bar{X}-\frac{n\bar{X}+m\bar{Y}}{n+m}\right)^2+m\left(\bar{Y}-\frac{n\bar{X}+m\bar{Y}}{n+m}\right)^2=\frac{2(n\xbar-m\ybar)^2}{(n+m)^2}+\frac{-(n^2\xbar^2+m^2\ybar^2)-4nm\xbar\ybar+nm\xbar^2+nm\ybar^2}{n+m}$$
$$=\frac{nm\xbar^2-2nm\xbar\ybar+nm\ybar^2}{n+m}=\left(1/n+1/m\right)^{-1}(\xbar-\ybar)^2$$
Dividing this by $s_p^2$ and moving the $\left(1/n+1/m\right)$ down to the bottom, taking the square root gives the $t$-statistic.
\subsection*{12.5}
I think we did this in class... but okay.

Under $H_0$, the observations $Y_{ij}$ are iid $N(\mu,\sigma^2)$. The log-likelihood function is 
$$l_0=-\frac{J_.}{2}\log(2\pi)-\frac{J_.}{2}\log\sigma^2-\frac{1}{2\sigma^2}\sum_i\sum_j(Y_{ij}-\mu)^2$$
Differentiating wrt $\mu$ gives $\frac{1}{2\sigma^2}\sum_i\sum_j2(Y_{ij}-\mu)=0$. Rearranging gives the MLE for $\mu$ as $\ybar$. Similarly, differentiating wrt $\sigma^2$ gives $-\frac{J_.}{2\sigma^2}+\frac{1}{2\sigma^4}\sum_i\sum_j(Y_{ij}-\hat{\mu})^2=0$, which gives $\hat{\sigma^2}=\frac{1}{J.}\sum_i\sum_j(Y_{ij}-\ybar)^2$.

Plugging these in gives the log-likelihood of $H_0$, which after some cancellation is 
$$l_0=-\frac{J.}{2}(\log(2\pi)+\log(\hat{\sigma^2})+1)$$

Under the general hypothesis in which the means of the groups is allowed to vary, we have 
$$l=-\frac{J_.}{2}\log(2\pi)-\frac{J_.}{2}\log\sigma^2-\frac{1}{2\sigma^2}\sum_i\sum_j(Y_{ij}-\mu_i)^2$$
When we differentiate wrt $\mu_k$, everything but the $k$th term in the outer sum drops out and we end up with $\frac{1}{\sigma^2}\sum_j(Y_{kj}-\mu_k)$, which gives $\hat{\mu_k}=\ybar_{k.}$. $\hat{\sigma}$ remains the same, since we have changed nothing of importance, but the $\ybar$ changes to a $\ybar_{k.}$. Plugging these in, we get 
$$l=-\frac{J.}{2}(\log(2\pi)+\log(\hat{\sigma^2})+1)$$

Plugging these in gives the log-likelihood of $H_0$, which after some cancellation is 
$$l_0=-\frac{J.}{2}(\log(2\pi)+\log(\hat{\sigma^2})+1)$$

Under the general hypothesis in which the means of the groups is allowed to vary, we have 
$$l=-\frac{J_.}{2}\log(2\pi)-\frac{J_.}{2}\log\sigma^2-\frac{1}{2\sigma^2}\sum_i\sum_j(Y_{ij}-\mu_i)^2$$
When we differentiate wrt $\mu_k$, everything but the $k$th term in the outer sum drops out and we end up with $\frac{1}{\sigma^2}\sum_j(Y_{kj}-\mu_k)$, which gives $\hat{\mu_k}=\ybar_{k.}$. $\hat{\sigma}$ remains the same, since we have changed nothing of importance, but the $\ybar$ changes to a $\ybar_{k.}$. Plugging these in, we get 
$$l=-\frac{J.}{2}(\log(2\pi)+\log(\hat{\sigma^2})+1)$$

The LRT statistic is 
$$-2\log\Lambda=-2(l_0-l)=J.(\log(\hat{\sigma^2_0}-\hat{\sigma^2}))=J.\log\left(\frac{\sum_i\sum_j(Y_{ij}-\ybar)^2}{\sum_i\sum_j(Y_{ij}-\mu_i)^2}\right)=J.\log\left(\frac{SS_w+SS_b}{SS_w}\right)=J.\log(SS_b/SS_w+1)$$
which follows a $\chi^2_{I-1}$ distribution. 

If we fix the significance level $\alpha$, we reject when $P(-2\log\lambda>c)<\alpha$ or when 
$$P(J.\log(SS_b/SS_w+1)>c)=P(SS_b/SS_w>\exp(c/J.)-1)=P\left(\frac{SS_b/(I-1)}{SS_w/(J.-I)}>\frac{J.-I}{I-1}(\exp(c/J.)-1)\right)<\alpha$$
This is just rejecting when the $F$-statistic is bigger than some critical value, and we can pick $c$ so that they reject at the same value.
\subsection*{12.9}
The book isn't very clear about this, but it seems like that the $t=2$ column corresponds to a range of two sample means, or essentially the absolute value of the difference of two sample means. The $q$-statistic is $\frac{|\xbar-\ybar|}{s_p/\sqrt{J}}$ whereas the $t$-statistic is $\frac{\xbar-\ybar}{s_p\sqrt{2/J}}$. Suppose we know that $P(q>q_0)=0.10$, where $q_0$ is the entry in the $q_{90}$ table. Then, 
$$P\left(\frac{|\xbar-\ybar|}{s_p/\sqrt{J}}>q_0\right)=P\left(\frac{|\xbar-\ybar|}{s_p\sqrt{2/J}}>q_0/\sqrt{2}\right)=0.10$$
so 
$$P(\left(\frac{\xbar-\ybar}{s_p/\sqrt{2J}}>q_0/\sqrt{2}\right)=P(t>q_0/\sqrt{2})=0.05$$
which means that at the same degree of freedom, the $90\%$level for $q$ of $2$ should correspond to the $95\%$ level for $t$.
\subsection*{12.20}
a. $$E(MS_W)=\frac{1}{I(J-1)}E(SS_W)=\frac{1}{I(J-1)}\sum_i\sum_jE((Y_{ij}-\bar{Y}_{i.})^2)$$
We can now use Lemma A. The expected value of each $Y_{ij}$ is $\mu+E(A_i)+E(\ep_{ij})=\mu$, and $\var(Y_{ij})=\var(A_i)+\var(\ep_{ij})=\sigma_\ep^2$ for a fixed $i$. The inner sum resolves to $(J-1)\sigma_\ep^2$, so summing over $i$ gives $I(J-1)\sigma_\ep^2$, which is just $\sigma_\ep^2$ when divided by $I(J-1)$.

Similarly, $E(MS_B)=\frac{J}{I-1}\sum_iE((\bar{Y}_{i.}-\bar{Y}_{..})^2)$. We can apply Lemma A again, where we have $E(\bar{Y}_{i.})=\frac{1}{J}\sum_jE(Y_{ij})=E(A_i)+\mu=\mu$ and $\var(\bar{Y}_{i.})=\frac{1}{J^2}\sum_j\sum_k\cov(Y_{ij},Y_{ik})=\frac{1}{J^2}\sum_j\sum_k(\sigma_A^2+\delta_{jk}\sigma_\ep^2)=\sigma_\ep^2/J+\sigma_A^2$, where $\delta$ denotes the Kronecker delta. Then, the inner sum resolves to $(I-1)(\sigma_\ep^2/J+\sigma_A^2)$, and multiplying by the thing out front gives us $\sigma_\ep^2+J\sigma_A^2$.

We can then estimate $\sigma_\ep^2$ as $MS_W$ and $\sigma_A^2$ as $(MS_B-MS_W)/J$. Calculating these for the data given, we have $MS_W=4.469$ and $MS_B=83.09$

\noindent b. This would be the two-way ANOVA layout with no interaction term, since we do not expect any sort of interaction between randomized subsamples and batches. By Theorem A in 12.3.2, we have $E(MS_E)=\frac{1}{IJ(K-1)}E(SS_E)=\sigma_\ep^2$, where $K$ is the number of duplicate measurements on each subsample. We also have that $E(MS_A)=\frac{1}{(I-1)}E(SS_A)=\sigma_\ep^2+JK\sigma_A^2$, where $A$ refers to the effect of the batches, and $E(MS_B)=\frac{1}{(J-1)}E(SS_B)=\sigma_\ep^2+IK\sigma_B^2$, where $B$ refers to the effect of the subsample variation.
\subsection*{12.21}
An ANOVA table:

\begin{tabular}{cccccc}
\hline
Source & SS & df & MS & F & p\\
\hline
Between & 27234 & 3 & 9078 & 2.2712 & 0.119\\
Within & 63953 & 16 & 3997 \\
Total & 91187 & 19 & 4799\\
\end{tabular}

The $p$-value suggests that there's not enough evidence to conclude that the means are different. A boxplot says the same thing, as they're all roughly around the same neighborhood.

\includegraphics[width=0.5\textwidth]{scripts_6/12_21.png}
\end{document}
